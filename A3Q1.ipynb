{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. CNN for Image Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Set up environment\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Prepare dataset\n",
    "# Define data transformations with feature scaling\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# Load the CIFAR-10 dataset\n",
    "Train_dataset = CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
    "Test_dataset = CIFAR10(root='./data', train=False, transform=transform, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Split dataset\n",
    "# define sizes of train, validation, test datasets\n",
    "train_size = int(0.6 * len(Train_dataset))\n",
    "val_size = int(0.2 * len(Train_dataset))\n",
    "test_size = int(0.2 * len(Train_dataset))\n",
    "\n",
    "\n",
    "# Splitting datasets\n",
    "train_dataset, val_dataset, test_dataset = random_split(Train_dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Convert datasets to DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Building CNN\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, x1, m1, x2, m2, x3, x4, x5, d):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, x1, kernel_size=m1, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(x1, x2, kernel_size=m2, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.fc1 = nn.Linear(x2 * 8 * 8, x3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(p=d)\n",
    "\n",
    "        self.fc2 = nn.Linear(x3, x4)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(p=d)\n",
    "\n",
    "        self.fc3 = nn.Linear(x4, x5)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout(p=d)\n",
    "\n",
    "        self.fc4 = nn.Linear(x5, 10)  # 10 classes for CIFAR-10\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "\n",
    "        x = x.view(-1, x2 * 8 * 8)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        x = self.fc4(x)\n",
    "        return self.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Determine the parameters\n",
    "x1, m1, x2, m2, x3, x4, x5, d = 32, 3, 64, 3, 4096, 512, 128, 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN(x1, m1, x2, m2, x3, x4, x5, d)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Train the model\n",
    "def train_model(model, train_loader, val_loader, epochs, lr):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training Step\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            # Backward and Optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        train_losses.append(running_loss / len(train_loader))\n",
    "\n",
    "        # Evaluation Step\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_losses.append(val_loss / len(val_loader))\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {train_losses[-1]}, Val Loss: {val_losses[-1]}')\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "train_losses, val_losses = train_model(model, train_loader, val_loader, epochs=20, lr=0.001)\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.plot(range(1, 21), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, 21), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Evaluate the Model\n",
    "model.eval()\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "conf_matrix = confusion_matrix(all_labels, all_preds)\n",
    "precision = precision_score(all_labels, all_preds, average='macro')\n",
    "recall = recall_score(all_labels, all_preds, average='macro')\n",
    "\n",
    "print(f'Test Accuracy: {accuracy*100} %')\n",
    "print(f'Confusion Matrix:\\n{conf_matrix}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Plot training and validation loss for different learning rates\n",
    "learning_rates = [0.0001, 0.001, 0.01, 0.1]\n",
    "\n",
    "fig = plt.figure(figsize=(20, 4))\n",
    "i = 0\n",
    "\n",
    "for lr in learning_rates:\n",
    "    print(f\"For Learning Rate = {lr}\")\n",
    "    model = CNN(x1, m1, x2, m2, x3, x4, x5, d)\n",
    "    train_losses, val_losses = train_model(model, train_loader, val_loader, epochs=8, lr=lr)\n",
    "\n",
    "    i += 1\n",
    "    fig.add_subplot(1,4,i)\n",
    "    plt.plot(range(1, 9), train_losses, label=f'Training Loss, LR={lr}')\n",
    "    plt.plot(range(1, 9), val_losses, label=f'Validation Loss, LR={lr}')\n",
    "    plt.title(f\"Learning Rate = {lr}\")\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
